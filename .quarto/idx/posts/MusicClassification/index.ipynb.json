{"title":"Deep Music Genre Classification","markdown":{"yaml":{"title":"Deep Music Genre Classification","author":"Liz Rightmire","date":"2024-05-08","image":"image.jpg","description":"Utilizing Neural Networks to Classify Song Genre","format":"html"},"headingText":"Deep Music Classification","containsRefs":false,"markdown":"\n\n\n### Introduction\n\nLike most of the population, I listen to music every day: when I'm in the shower, when I'm walking to class, or when I want to process a specific emotion. I enjoy a variety of different genres, especially pop and country. If you played me a pop or country song, I don't think I'd have a challenge classifying it as one of the two. Genres of music tend to have similar word choice in the lyrics, similar levels of \"upbeat-ness,\" similar content matters.\n\nIn this blog post, I will use Torch to predict the genre of a song based on the track’s lyrics and engineered features.\n\nI will create three neural networks using torch and train them, evaluating each one using unseen validation data.\n\nThe first neural network will only use the song lyrics, the second will use engineered features, such as 'family/gospel', 'romantic', and 'obscene,' which contain numerical ratings of how these content matters apply to the song. My third network will use both the lyrics and the engineered features. \n\nFinally, I will investigate the word embeddings learned by my models and consider what biases my model has learned based on songs' content.\n\n### Data Preparation\n\nLoad in dataset. We have 31 observations of 28372 songs.\n\nThere are 7 genres we'll classify into.\n\nLabel encode the genres\n\nWhat would be the baseline accuracy for our model?\n\nIf our model always predicts a song to be pop, it would achieve 24% accuracy. Let's see if we can beat this using neural networks.\n\n### Neural Network 1: Lyrics\n\nThis class allows us to retrieve the lyrics from the data frame\n\nTrain test split\n\nInvestigate an entry of our training dataset. Each entry has the lyrics, as well as the engineered features\n\nTo tokenize the text, each word is assigned an integer value. This allows us to feed the lyrics into a neural netowrk.\n\nTo convert between the tokens and the word, create a yield_tokens method.\n\nLet's check out of vocabulary.\n\nHow does this look as tokens? We see the words as their integer representation.\n\nMake a text pipeline to preprocess text data\n\nMethod collate_batch processes a batch of data\n\nCreate two instances of data loaders for train and test set.\n\nNow we can build our model! First, embed the text, then utilize dropout to prevent neuron dependency, and finish with a fully-connected linear layer.\n\nSet parameters for model training\n\nHere is the training loop and evaluation method we'll use for each neural network.\n\nNow we can finally train the model. In just 5 epochs we receive 56% training set accuracy.\n\nEvaluate our model on a testing dataset. 32% validation accuracy is better than the baseline!\n\n### Neural Network 2: Engineered Features\n\nWe were able to effectively classify song genre using just the lyrics. Can we do the same using engineered features -- descriptors of topics relavent to the songs?\n\nOur Engineering Classification Model utilizes sequences of Linear, ReLU, and Dropout layers.\n\nLet's train and test.\n\nThis is only slightly better than baseline. Perhaps lyrics are better and predicting genre, or the model I designedis not strong enough for this task.\n\n### Neural Network 3: Lyrics and Engineered Featuers \n\nCan we get high accuracy by combining our previous two neural networks -- using both lyrics and engineered features to classify song genre?\n\nOnce again, slighly better than baseline!\n\n### Visualize Word Embedding\n\nText embedding models blindly learn associations between words used in the input text. It would be unsurprising to see this occur in songs, especially considering the tendency for music, especially hip-hop and country, to contain racist and sexist undertones.\n\nLet's utilize PCA to reduce the dimensionality of our data\n\n![PCA graph](pca.jpg)\n\nIt's a bit hard to make sense of, seeing that we are classifying into 7 categories. However, some of the \"outlier\" words in our PCA plot seem to be associated with specific genres. \n\n![word embedding image](word_embeddings.jpg)\n\nI'm pleased to see not too much association between stereotypically feminine traits and unequivocally feminine words, and same with masculine traits and unequivocally masculine words. \n\n### Conclusion\n\nIn this blog post, I learned to design neural networks to handle different types of inputs: text input (lyrics), and engineered features. Utilizing machine learning, I was able to create classification models for 7 categories, performing better than the baseline. Finally, I analyzed the word embeddings learned by my model, considering how these came to be.\n\nNeural network models are ubiquitous -- perhaps the most commonly used machine learning algorithm. Every time I use fingerprint ID on my phone, or use auto-complete, I'm utilizing neural networks. It's exciting to begin to understand how these work, and apply them in an interesting way -- to music!\n","srcMarkdownNoYaml":"\n\n# Deep Music Classification\n\n### Introduction\n\nLike most of the population, I listen to music every day: when I'm in the shower, when I'm walking to class, or when I want to process a specific emotion. I enjoy a variety of different genres, especially pop and country. If you played me a pop or country song, I don't think I'd have a challenge classifying it as one of the two. Genres of music tend to have similar word choice in the lyrics, similar levels of \"upbeat-ness,\" similar content matters.\n\nIn this blog post, I will use Torch to predict the genre of a song based on the track’s lyrics and engineered features.\n\nI will create three neural networks using torch and train them, evaluating each one using unseen validation data.\n\nThe first neural network will only use the song lyrics, the second will use engineered features, such as 'family/gospel', 'romantic', and 'obscene,' which contain numerical ratings of how these content matters apply to the song. My third network will use both the lyrics and the engineered features. \n\nFinally, I will investigate the word embeddings learned by my models and consider what biases my model has learned based on songs' content.\n\n### Data Preparation\n\nLoad in dataset. We have 31 observations of 28372 songs.\n\nThere are 7 genres we'll classify into.\n\nLabel encode the genres\n\nWhat would be the baseline accuracy for our model?\n\nIf our model always predicts a song to be pop, it would achieve 24% accuracy. Let's see if we can beat this using neural networks.\n\n### Neural Network 1: Lyrics\n\nThis class allows us to retrieve the lyrics from the data frame\n\nTrain test split\n\nInvestigate an entry of our training dataset. Each entry has the lyrics, as well as the engineered features\n\nTo tokenize the text, each word is assigned an integer value. This allows us to feed the lyrics into a neural netowrk.\n\nTo convert between the tokens and the word, create a yield_tokens method.\n\nLet's check out of vocabulary.\n\nHow does this look as tokens? We see the words as their integer representation.\n\nMake a text pipeline to preprocess text data\n\nMethod collate_batch processes a batch of data\n\nCreate two instances of data loaders for train and test set.\n\nNow we can build our model! First, embed the text, then utilize dropout to prevent neuron dependency, and finish with a fully-connected linear layer.\n\nSet parameters for model training\n\nHere is the training loop and evaluation method we'll use for each neural network.\n\nNow we can finally train the model. In just 5 epochs we receive 56% training set accuracy.\n\nEvaluate our model on a testing dataset. 32% validation accuracy is better than the baseline!\n\n### Neural Network 2: Engineered Features\n\nWe were able to effectively classify song genre using just the lyrics. Can we do the same using engineered features -- descriptors of topics relavent to the songs?\n\nOur Engineering Classification Model utilizes sequences of Linear, ReLU, and Dropout layers.\n\nLet's train and test.\n\nThis is only slightly better than baseline. Perhaps lyrics are better and predicting genre, or the model I designedis not strong enough for this task.\n\n### Neural Network 3: Lyrics and Engineered Featuers \n\nCan we get high accuracy by combining our previous two neural networks -- using both lyrics and engineered features to classify song genre?\n\nOnce again, slighly better than baseline!\n\n### Visualize Word Embedding\n\nText embedding models blindly learn associations between words used in the input text. It would be unsurprising to see this occur in songs, especially considering the tendency for music, especially hip-hop and country, to contain racist and sexist undertones.\n\nLet's utilize PCA to reduce the dimensionality of our data\n\n![PCA graph](pca.jpg)\n\nIt's a bit hard to make sense of, seeing that we are classifying into 7 categories. However, some of the \"outlier\" words in our PCA plot seem to be associated with specific genres. \n\n![word embedding image](word_embeddings.jpg)\n\nI'm pleased to see not too much association between stereotypically feminine traits and unequivocally feminine words, and same with masculine traits and unequivocally masculine words. \n\n### Conclusion\n\nIn this blog post, I learned to design neural networks to handle different types of inputs: text input (lyrics), and engineered features. Utilizing machine learning, I was able to create classification models for 7 categories, performing better than the baseline. Finally, I analyzed the word embeddings learned by my model, considering how these came to be.\n\nNeural network models are ubiquitous -- perhaps the most commonly used machine learning algorithm. Every time I use fingerprint ID on my phone, or use auto-complete, I'm utilizing neural networks. It's exciting to begin to understand how these work, and apply them in an interesting way -- to music!\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","title-block-banner":"../../img/landscape.png","title-block-banner-color":"white","theme":"cosmo","title":"Deep Music Genre Classification","author":"Liz Rightmire","date":"2024-05-08","image":"image.jpg","description":"Utilizing Neural Networks to Classify Song Genre"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}