{"title":"Women in Data Science Conference","markdown":{"yaml":{"title":"Women in Data Science Conference","author":"Liz Rightmire","date":"2024-3-10","image":"image.jpg","description":"Researching gender disparities in data science, and attending Women in Data Science Conference","format":"html"},"headingText":"Women in Data Science Conference","containsRefs":false,"markdown":"\n\n\n### Abstract\n\nWomen are underrepresented in computing, math, and engineering, making up just 26% of computing professionals and 12% of engineering professionals in 2013. In this blog post, I aim to investigate how this gender divide developed, consider techniques proven to reverse this disparity, and apply my learning to the Middlebury College Women in Data Science Conference.\n\n### Introduction\n\nFemale underrepresentation in computing is problematic for many reasons. First, group dynamics in teams without women suffer, as women are “more likely than men to prioritize helping and working with other people over career goals” (Corbett & Hill, 4). The female perspective brings diversity to the workforce, contributing to creativity, productivity, and innovation. The biggest issues of this century – climate change, disease, overpopulation – require the skills of engineers and computer scientists. Everyone misses out on novel solutions when diverse participation is not prioritized. Underrepresentation of women in engineering fields is becoming a global issue, too; the US is less globally competitive because it is not hiring the best people for the jobs.\n\nIt wasn’t always the case that female voices were scarce in these technical fields; women were significant players in the early decades of computing. Ada Lovelace was one of the early pioneers in the 1880s, and numerous women were programmers during World War II (Abbate). In the 1950s and 1960s, women maintained a presence in computing — representing about a quarter of workers. This statistic grew steadily until 1990, when just over a third of computing workers were women. At this time, computing lacked a gender identity and attracted women as well as men. However, women made up just 26% of computing professionals in 2013. “Over a relatively short period of time, a field that was once relatively gender-integrated has become solidly male-dominated” (Koput & Gutek, 105). What caused a decrease in women’s representation from 1990? One explanation lies in hiring practices that favored men, such as aptitude and personality tests which privileged male stereotyped characteristics — antisocial, mathematically inclined programmers were considered to be the best. Also, as personal computers became more readily available, young boys gained dominant ownership over these toys and the association of men and computers was fostered. As programming curriculums became more lucrative at colleges, universities imposed stringent requirements for entry and completion of the major, disadvantaging women who entered college with less programming experience after being encouraged to study things like “home economics” if they were found to be engineering-minded (Bix, 2002). \n\nHowever, the representation of women in computing can improve and is improving. At Harvey Mudd College, the percentage of women graduating from computing grew from 12% to 40% in five years, brought about by a revision of the introductory course, research opportunities for undergraduates, and taking female students to a Women in Computing Conference (Corbett & Hill, 4). There are many reasons why hosting events that spotlight women in STEM is critical for female engineering representation. Women in engineering and computing often report isolation, a lack of voice, and a lack of support, whereas men cite ample social networks in computing (Corbett & Hill, 34). It is important to create spaces that allow women to network and support each other, as a sense of belonging has measurable effects on an individual’s physical and mental state. The stereotype that women are less competent programmers is incredibly dangerous. Stereotype bias inhibits women when “individuals fear that they will confirm a negative stereotype about a group to which they belong” (Corbett & Hill, 3). These harmful stereotypes make women less confident and therefore hurt their performance. In the modern decade, explicit gender bias is relatively rare; what is more problematic is implicit gender bias, which can be combated only by individuals of all genders attending events that spotlight women in STEM.\n\n### Conference Speakers\n\nAn example of such an event is the Women in Data Science Conference at Middlebury College. This year, in 2024, three female professors from different academic departments presented lightning talks on how they use data science in their research projects, three female alumni spoke on their unique careers in data science, and Professor Sarah Brown visited from the University of Rhode Island to speak on her research concerning fairness in machine learning.\n\nThe first lightning speaker was Professor Laura Beister from the computer science department, who presented her Ph.D. research. Professor Beister uses natural language processing to analyze Reddit posts of individuals who self-identify themselves online as depressed. This research analyzes posts on Reddit; while this may seem unreliable, it is true that people tend to post anonymously on Reddit about their conditions, known as an “identity claim,” and be truthful in such posts. Reddit is an incredibly large body of text, so Professor Beister used heavy computing power to search through the postings, find relevant ones, and then convert them into vectorized representations that she could perform analysis. She found that depressed individuals' language markers are similar to those of anxious, sad individuals, but more closely match controls (non-depressed individuals) after they make an identity claim as a depressed individual online. This indicates that there may be benefits to sharing one’s depression diagnosis, especially in a semi-anonymous form where people are likely to be empathetic. \n\nNext, Professor Amy Yuen presented her work modeling behavior in the UN Security Council to examine when the council decides to take up an issue and how the institution affects policy coordination. I learned that the UN Security Council is one of the six principal components of the United Nations and is tasked with recommending the admission of new UN members to the General Assembly and approving any changes to the UN Charter. Amy Yuen gathered data from different actors in the UN Security Council, a time-intensive process that relied heavily on data cleansing. She then used data science to argue for a different approach to considering the council’s decisions: a legislative bargaining framework. While I struggled to understand some of the principles surrounding international conflict resolution, I came out of this talk with an appreciation for how gathering and cleansing data can prove to be the most challenging, time-intensive part of the research process, and I began thinking about the skills I’ve learned in CS classes and how they may aid in this process.\n\nThe last lightning speaker was Professor Jessica L’Roe, who spoke on one of her research projects concerning land access and trends of inequality in the highlands of East Africa. One of her research projects that I found particularly interesting surveyed fifty rural women about their strategies and challenges investing in children’s future livelihoods via land. She found that over 80% of women thought investing in education was more worthwhile than land. I was inspired to learn more about her path to becoming a woman in geography research, and was appreciated her reflection on gender stereotypes that she came to reject later in her life. She also touched on the challenge of gathering data in developing countries, where the results of the research are celebrated in the academic world but may have a relatively low impact on the communities they analyze.\n\nFinally, the visiting speaker, Dr. Professor Sarah Brown, gave an in-depth talk about her path to becoming a professor of data science. She described machine learning as a combination of computer science, statistics, and domain expertise; a description I’d never heard before. Professor Brown emphasized the importance of understanding problems in a nuanced way by soliciting the advice of industry professionals before trying to apply data science. A complex understanding of bias and consideration of fairness must occur before a data scientist ever considers building a model. This is why Professor Sarah Brown designed her curriculum to teach students about fairness before ever teaching them how to fit a machine learning model.\nProfessor Brown's path to professorship was interesting and dynamic. In her career, she has learned “keys” that help her succeed. The first is understanding data’s context. She shared an anecdote about how diving deeply into a dataset allowed her to modify a regression problem to include classification in the cost function, producing better results for PTSD diagnosis. Next, a data scientist must consider disciplines as communities. Understanding data and a problem requires soliciting advice from people in all fields. Finally, Professor Brown spoke on the importance of meeting people where they are. A data scientist must be able to present technical findings to a non-technical audience, and explaining complex models in a flamboyant way only reduces its impact.\n\n### Conclusion\n\nI found it valuable to learn more about the presence of gender disparities in a field I so passionately want to break into, and then take this knowledge to the Women in Data Science Conference. While it was at times upsetting to learn about the deep-rooted history of these inequalities, I now have a calm understanding of the facts that I can leverage to support myself if I ever find myself in a frustrating situation. I also now know techniques that can be used to reverse the statistics, of “solve the equation.” I’m inspired by the incredible work of women in data science that was showcased at this event, and I aspire to someday join the ranks of bright women in this field.\n","srcMarkdownNoYaml":"\n\n# Women in Data Science Conference\n\n### Abstract\n\nWomen are underrepresented in computing, math, and engineering, making up just 26% of computing professionals and 12% of engineering professionals in 2013. In this blog post, I aim to investigate how this gender divide developed, consider techniques proven to reverse this disparity, and apply my learning to the Middlebury College Women in Data Science Conference.\n\n### Introduction\n\nFemale underrepresentation in computing is problematic for many reasons. First, group dynamics in teams without women suffer, as women are “more likely than men to prioritize helping and working with other people over career goals” (Corbett & Hill, 4). The female perspective brings diversity to the workforce, contributing to creativity, productivity, and innovation. The biggest issues of this century – climate change, disease, overpopulation – require the skills of engineers and computer scientists. Everyone misses out on novel solutions when diverse participation is not prioritized. Underrepresentation of women in engineering fields is becoming a global issue, too; the US is less globally competitive because it is not hiring the best people for the jobs.\n\nIt wasn’t always the case that female voices were scarce in these technical fields; women were significant players in the early decades of computing. Ada Lovelace was one of the early pioneers in the 1880s, and numerous women were programmers during World War II (Abbate). In the 1950s and 1960s, women maintained a presence in computing — representing about a quarter of workers. This statistic grew steadily until 1990, when just over a third of computing workers were women. At this time, computing lacked a gender identity and attracted women as well as men. However, women made up just 26% of computing professionals in 2013. “Over a relatively short period of time, a field that was once relatively gender-integrated has become solidly male-dominated” (Koput & Gutek, 105). What caused a decrease in women’s representation from 1990? One explanation lies in hiring practices that favored men, such as aptitude and personality tests which privileged male stereotyped characteristics — antisocial, mathematically inclined programmers were considered to be the best. Also, as personal computers became more readily available, young boys gained dominant ownership over these toys and the association of men and computers was fostered. As programming curriculums became more lucrative at colleges, universities imposed stringent requirements for entry and completion of the major, disadvantaging women who entered college with less programming experience after being encouraged to study things like “home economics” if they were found to be engineering-minded (Bix, 2002). \n\nHowever, the representation of women in computing can improve and is improving. At Harvey Mudd College, the percentage of women graduating from computing grew from 12% to 40% in five years, brought about by a revision of the introductory course, research opportunities for undergraduates, and taking female students to a Women in Computing Conference (Corbett & Hill, 4). There are many reasons why hosting events that spotlight women in STEM is critical for female engineering representation. Women in engineering and computing often report isolation, a lack of voice, and a lack of support, whereas men cite ample social networks in computing (Corbett & Hill, 34). It is important to create spaces that allow women to network and support each other, as a sense of belonging has measurable effects on an individual’s physical and mental state. The stereotype that women are less competent programmers is incredibly dangerous. Stereotype bias inhibits women when “individuals fear that they will confirm a negative stereotype about a group to which they belong” (Corbett & Hill, 3). These harmful stereotypes make women less confident and therefore hurt their performance. In the modern decade, explicit gender bias is relatively rare; what is more problematic is implicit gender bias, which can be combated only by individuals of all genders attending events that spotlight women in STEM.\n\n### Conference Speakers\n\nAn example of such an event is the Women in Data Science Conference at Middlebury College. This year, in 2024, three female professors from different academic departments presented lightning talks on how they use data science in their research projects, three female alumni spoke on their unique careers in data science, and Professor Sarah Brown visited from the University of Rhode Island to speak on her research concerning fairness in machine learning.\n\nThe first lightning speaker was Professor Laura Beister from the computer science department, who presented her Ph.D. research. Professor Beister uses natural language processing to analyze Reddit posts of individuals who self-identify themselves online as depressed. This research analyzes posts on Reddit; while this may seem unreliable, it is true that people tend to post anonymously on Reddit about their conditions, known as an “identity claim,” and be truthful in such posts. Reddit is an incredibly large body of text, so Professor Beister used heavy computing power to search through the postings, find relevant ones, and then convert them into vectorized representations that she could perform analysis. She found that depressed individuals' language markers are similar to those of anxious, sad individuals, but more closely match controls (non-depressed individuals) after they make an identity claim as a depressed individual online. This indicates that there may be benefits to sharing one’s depression diagnosis, especially in a semi-anonymous form where people are likely to be empathetic. \n\nNext, Professor Amy Yuen presented her work modeling behavior in the UN Security Council to examine when the council decides to take up an issue and how the institution affects policy coordination. I learned that the UN Security Council is one of the six principal components of the United Nations and is tasked with recommending the admission of new UN members to the General Assembly and approving any changes to the UN Charter. Amy Yuen gathered data from different actors in the UN Security Council, a time-intensive process that relied heavily on data cleansing. She then used data science to argue for a different approach to considering the council’s decisions: a legislative bargaining framework. While I struggled to understand some of the principles surrounding international conflict resolution, I came out of this talk with an appreciation for how gathering and cleansing data can prove to be the most challenging, time-intensive part of the research process, and I began thinking about the skills I’ve learned in CS classes and how they may aid in this process.\n\nThe last lightning speaker was Professor Jessica L’Roe, who spoke on one of her research projects concerning land access and trends of inequality in the highlands of East Africa. One of her research projects that I found particularly interesting surveyed fifty rural women about their strategies and challenges investing in children’s future livelihoods via land. She found that over 80% of women thought investing in education was more worthwhile than land. I was inspired to learn more about her path to becoming a woman in geography research, and was appreciated her reflection on gender stereotypes that she came to reject later in her life. She also touched on the challenge of gathering data in developing countries, where the results of the research are celebrated in the academic world but may have a relatively low impact on the communities they analyze.\n\nFinally, the visiting speaker, Dr. Professor Sarah Brown, gave an in-depth talk about her path to becoming a professor of data science. She described machine learning as a combination of computer science, statistics, and domain expertise; a description I’d never heard before. Professor Brown emphasized the importance of understanding problems in a nuanced way by soliciting the advice of industry professionals before trying to apply data science. A complex understanding of bias and consideration of fairness must occur before a data scientist ever considers building a model. This is why Professor Sarah Brown designed her curriculum to teach students about fairness before ever teaching them how to fit a machine learning model.\nProfessor Brown's path to professorship was interesting and dynamic. In her career, she has learned “keys” that help her succeed. The first is understanding data’s context. She shared an anecdote about how diving deeply into a dataset allowed her to modify a regression problem to include classification in the cost function, producing better results for PTSD diagnosis. Next, a data scientist must consider disciplines as communities. Understanding data and a problem requires soliciting advice from people in all fields. Finally, Professor Brown spoke on the importance of meeting people where they are. A data scientist must be able to present technical findings to a non-technical audience, and explaining complex models in a flamboyant way only reduces its impact.\n\n### Conclusion\n\nI found it valuable to learn more about the presence of gender disparities in a field I so passionately want to break into, and then take this knowledge to the Women in Data Science Conference. While it was at times upsetting to learn about the deep-rooted history of these inequalities, I now have a calm understanding of the facts that I can leverage to support myself if I ever find myself in a frustrating situation. I also now know techniques that can be used to reverse the statistics, of “solve the equation.” I’m inspired by the incredible work of women in data science that was showcased at this event, and I aspire to someday join the ranks of bright women in this field.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","title-block-banner":"../../img/landscape.png","title-block-banner-color":"white","theme":"cosmo","title":"Women in Data Science Conference","author":"Liz Rightmire","date":"2024-3-10","image":"image.jpg","description":"Researching gender disparities in data science, and attending Women in Data Science Conference"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}