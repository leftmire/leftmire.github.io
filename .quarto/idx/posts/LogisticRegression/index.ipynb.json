{"title":"Implementing Logistic Regression","markdown":{"yaml":{"title":"Implementing Logistic Regression","author":"Liz Rightmire","date":"2024-04-10","image":"image.jpg","description":"Implementing and Testing Logistic Regression","format":"html"},"headingText":"Logistic Regression Implementation","containsRefs":false,"markdown":"\n\n\nSource code for [Logistc.py](https://github.com/leftmire/lizrightmireio.github.io/blob/main/posts/LogisticRegression/logistic.py)\n\n### Abstract\n\nLogistic Regression is a popular machine learning algorithm most commonly used in classification problems. In this blog post, I will create an object-oriented implementation of Logistic Regression. Then, I will perform a series of experiments to test the efficacy of Logistic Regression in different scenarios. First I consider vanilla gradient descent, in which the momentum term $\\beta$ is set to 0. Next I adjust $\\beta$ to a positive, nonzero value to see how introducing momentum into the algorithm allows for faster convergence. Finally, I investigate overfitting in Logistic Regression, seeing how a model can have a training accuracy of 100%, yet only 86% accuracy on a separate, testing dataset.\n\n### Experiments\n\n### Vanilla Gradient Descent\n\nTo begin, I will implement \"vanilla\" gradient descent, in which the momentum term, $\\beta$, is 0. Referring to the equation below, setting $\\beta$ to 0 cancels out the last term. The step in the gradient descent algorithm is computed as $\\alpha$ * the gradient of the loss function.  \n$\\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1}) $\n\nFirst, define functions to generate and plot our classification data, and one to draw the decision boundary we'll find next.\n\nPlot the classification data we'll work with\n\nWe have two distinct classes of points, and our goal is to find a decision boundary between them. \n\nWe can now train our logistic model and follow our progress by plotting the loss vs. the current iteration. We'll save the final weight vector before the loop terminates to plot the decision boundary.\n\nThe decision boundary found using vanilla gradient descent is a perfect match!\n\n### Benefits of Momentum\n\nVanilla gradient descent was very effective, but we can do better. Adjusting the momentum term $\\beta$ to a value other than 0 allows gradient descent to converge in fewer iterations. \n\n$\\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1}) $\n\nNow that $\\beta$ is not 0, The final term, or momentum term, is factored into the step calculation. The size of the difference between the current weights and previous iteration's weights have an impact on the size of the step.\n\nLet's prove this to ourselves.\n\nTrain a logistic regression model with $\\beta$ = 0.9\n\nThe green line represents vanilla gradient descent, whereas the red line is gradient descent with momentum. With momentum, we see significant improvements -- the initial loss is much lower, and the loss stays lower than vanilla gradient descent for each iteration up until 1,500. \n\n### Overfitting\n\nLogistic Regression models have the tendency to overfit. Let's explore this pattern.\n\nGenerate train and test data where p_dim > n_points.\n\nFit the model on the training data\n\nWe obtain an accuracy of 100% on the training data. This should raise concern that we may have oveffit!\n\nIndeed, accuracy is much lower -- 0.84 -- on the testing data.\n\n### Discussion\n\nIn this blog post, I set out to perform a deeper dive into a stalwart in the machine larning world: logistic regression. Instead of relying on sklearn's implementation as I have done in the past, I built my own logistic regression model and tested it in a variety of scenarios. The intricacies of the gradient and loss equations became more concrete as I toyed with tensors to produce the desired result. \n\nBeginning with vanilla gradient descent, I observed how the loss fell to zero and the decision bounary was stored in weights. Then, I dove into the theory behind momentum (how and why does it work?) and set $\\beta$ to 0.9 such that the momentum term of the step function would be at play. Finally, I saw how logistic regression can overfit -- something I'll be wary of in future projects. \n","srcMarkdownNoYaml":"\n\n# Logistic Regression Implementation\n\nSource code for [Logistc.py](https://github.com/leftmire/lizrightmireio.github.io/blob/main/posts/LogisticRegression/logistic.py)\n\n### Abstract\n\nLogistic Regression is a popular machine learning algorithm most commonly used in classification problems. In this blog post, I will create an object-oriented implementation of Logistic Regression. Then, I will perform a series of experiments to test the efficacy of Logistic Regression in different scenarios. First I consider vanilla gradient descent, in which the momentum term $\\beta$ is set to 0. Next I adjust $\\beta$ to a positive, nonzero value to see how introducing momentum into the algorithm allows for faster convergence. Finally, I investigate overfitting in Logistic Regression, seeing how a model can have a training accuracy of 100%, yet only 86% accuracy on a separate, testing dataset.\n\n### Experiments\n\n### Vanilla Gradient Descent\n\nTo begin, I will implement \"vanilla\" gradient descent, in which the momentum term, $\\beta$, is 0. Referring to the equation below, setting $\\beta$ to 0 cancels out the last term. The step in the gradient descent algorithm is computed as $\\alpha$ * the gradient of the loss function.  \n$\\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1}) $\n\nFirst, define functions to generate and plot our classification data, and one to draw the decision boundary we'll find next.\n\nPlot the classification data we'll work with\n\nWe have two distinct classes of points, and our goal is to find a decision boundary between them. \n\nWe can now train our logistic model and follow our progress by plotting the loss vs. the current iteration. We'll save the final weight vector before the loop terminates to plot the decision boundary.\n\nThe decision boundary found using vanilla gradient descent is a perfect match!\n\n### Benefits of Momentum\n\nVanilla gradient descent was very effective, but we can do better. Adjusting the momentum term $\\beta$ to a value other than 0 allows gradient descent to converge in fewer iterations. \n\n$\\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1}) $\n\nNow that $\\beta$ is not 0, The final term, or momentum term, is factored into the step calculation. The size of the difference between the current weights and previous iteration's weights have an impact on the size of the step.\n\nLet's prove this to ourselves.\n\nTrain a logistic regression model with $\\beta$ = 0.9\n\nThe green line represents vanilla gradient descent, whereas the red line is gradient descent with momentum. With momentum, we see significant improvements -- the initial loss is much lower, and the loss stays lower than vanilla gradient descent for each iteration up until 1,500. \n\n### Overfitting\n\nLogistic Regression models have the tendency to overfit. Let's explore this pattern.\n\nGenerate train and test data where p_dim > n_points.\n\nFit the model on the training data\n\nWe obtain an accuracy of 100% on the training data. This should raise concern that we may have oveffit!\n\nIndeed, accuracy is much lower -- 0.84 -- on the testing data.\n\n### Discussion\n\nIn this blog post, I set out to perform a deeper dive into a stalwart in the machine larning world: logistic regression. Instead of relying on sklearn's implementation as I have done in the past, I built my own logistic regression model and tested it in a variety of scenarios. The intricacies of the gradient and loss equations became more concrete as I toyed with tensors to produce the desired result. \n\nBeginning with vanilla gradient descent, I observed how the loss fell to zero and the decision bounary was stored in weights. Then, I dove into the theory behind momentum (how and why does it work?) and set $\\beta$ to 0.9 such that the momentum term of the step function would be at play. Finally, I saw how logistic regression can overfit -- something I'll be wary of in future projects. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","title-block-banner":"../../img/landscape.png","title-block-banner-color":"white","theme":"cosmo","title":"Implementing Logistic Regression","author":"Liz Rightmire","date":"2024-04-10","image":"image.jpg","description":"Implementing and Testing Logistic Regression"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}