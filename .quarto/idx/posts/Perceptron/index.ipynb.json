{"title":"Implementing the Perceptron Algorithm","markdown":{"yaml":{"title":"Implementing the Perceptron Algorithm","author":"Liz Rightmire","date":"2024-03-27","image":"image.jpg","description":"Implementing and Testing the Perceptron Algorithm","format":"html"},"headingText":"Implementing the Perceptron Algorithm","containsRefs":false,"markdown":"\n\n\n[Perceptron.py](https://github.com/leftmire/lizrightmireio.github.io/blob/main/posts/Perceptron/perceptron.py)\n\n\n[PerceptronMini.py](https://github.com/leftmire/lizrightmireio.github.io/blob/main/posts/Perceptron/perceptron_mini.py)\n\n### Abstract\n\nThis blog is a deep dive into the perceptron algorithm -- a supervised binary classification algorithm created in 1957. This algorithm begins with a randomly generated weight vector, and then updates the weight vectors using a random data point and a score generated from the point and the current weight vector. When the loss function quantifying the discrepency between predicted values of the data and actual values is 0, the algorithm terminates and the resulting weight vector describes the decision boundary between the classes.\n\nI begin by describing my implementation of a perceptron algorithm, and then  investigate the algorithm's effectiveness on three types of datasets: linearly separable, not linearly separable, and multi-dimensional. As expected, the algorithm is able to find the decision boundary between linearly separable classes, yet fails with not linearly separable classes. Finally, the five dimensional data I generate does not appear to be  separable via a perceptron algorithm.\n\n### Perceptron Implementation\n\nYou can find my implementation of the perceptron algorithm at [Perceptron.py](https://github.com/leftmire/lizrightmireio.github.io/blob/main/posts/Perceptron/perceptron.py). \n\nPerceptron.grad first computes the score for each data point in the feature matrix. The score is the feature matrix X matrix multiplied by the weight vector.\n\nIf the score is a different sign from the actual class label, the update is computed. Else, zeros are returned -- this entry in the weight vector doesn't need to be updated.\n\n### Experiment 1: Linearly Separable\n\nWhen given linearly separable data, the perceptron algorithm should converge to a weight vector that describes a line separating the two classes of data. Let's check that this is true for our implementation of the perceptron algorithm.\n\nFirst, generate linearly separable data\n\ncode modified from [Prof Phil's Notes](https://www.philchodrow.prof/ml-notes/chapters/20-perceptron.html#fig-demonstration)\n\nIt is easy to imagine a decision boundary that would divide these two classes.\n\nNow, let's run the perceptron algorithm. After just 3 iterations, the loss is 0 and a dividing line is discovered.\n\nHow did this happen? Using a simple loop, the answer to this question can be visualized on side-by-side plots. The circled point is the one randomly chosen for the perceptron update, and the line updates from the dotted line to the solid line.\n\nvisualization code modified from [Prof Phil's Notes](https://www.philchodrow.prof/ml-notes/chapters/20-perceptron.html#fig-demonstration)\n\n### Experiment 2: Not Linearly Separable\n\nLet's generate some data that isn't linearly separable by increasing the noise of our original data\n\nNow, let's run the perceptron algorithm\n\nEven with 1,000 perceptron updates, the algorithm does not terminate because the loss will never reach 0!\n\nWhat dividing line does the perceptron algorithm produce, 1,000 updates later?\n\nHmm, this isn't even a very good separator. Obviously, the perceptron algorithm performs poorly on data that isn't perfect linearly separable.\n\n### Experiment 3: Multi-Dimensional Data\n\nThe perceptron algorithm should work on data that is more than 2-dimensional.\n\nFirst, geneate some five-dimensional data to experiment with.\n\nNow, run the perceptron algorithm.\n\nEven after 3,000 iterations, the loss stays fairly high -- around 0.3. The loss does not appear to be consistently decreasing with each update. This leads me to believe that the data I created isn't linearly separable.\n\n### Minibatch Perceptron Experiments\n\nI edited my perceptron.py file to be compatable with the mini-batch perceptron algorithm, which computes an update using k points at once rathar than a single point.\n\nNow, I will conduct experiments to explore how minibatch perceptron performs in different scenarios. First, set k = 1.\n\nWith k = 1, minibatch performs similarly to the regular perceptron. The loss tends to decrease until reaching 0.\n\nLet's try k = 10.\n\nWhen k = 10, minibatch perceptron can still find a separating line in 2d.\n\nNow, let's experiment with k = n -- the batch size is the entire data set\n\nWhen setting k = n with data that is not linearly separable, the minibatch perceptron algorithm converges with a loss close to 0.\n\n### Conclusion\n\nThe update in each iteration of the perceptron algorithm is computed as X \\* w, the product between a single row of matrix X and the weight vector w. Each row in X has p entries for the p features, so this process is O(p). \n\nThe mini-batch perceptron algorithm takes the dot product between each data point in the batch, so the runtime is O(kp) for each iteration.\n\nI was able to prove that the perceptron algorithm, though simple, is both effective and efficient at finding a linear classifier for data of any dimension. I wrote my first implementation of an algorithm and gained a deeper understanding of its inner workings. I then took my understanding further and implemented the more efficient mini-batch perceptron with improved convergence.\n\nThis knowledge will translate well when I study more complicated machine learning algorithms. \n","srcMarkdownNoYaml":"\n\n# Implementing the Perceptron Algorithm\n\n[Perceptron.py](https://github.com/leftmire/lizrightmireio.github.io/blob/main/posts/Perceptron/perceptron.py)\n\n\n[PerceptronMini.py](https://github.com/leftmire/lizrightmireio.github.io/blob/main/posts/Perceptron/perceptron_mini.py)\n\n### Abstract\n\nThis blog is a deep dive into the perceptron algorithm -- a supervised binary classification algorithm created in 1957. This algorithm begins with a randomly generated weight vector, and then updates the weight vectors using a random data point and a score generated from the point and the current weight vector. When the loss function quantifying the discrepency between predicted values of the data and actual values is 0, the algorithm terminates and the resulting weight vector describes the decision boundary between the classes.\n\nI begin by describing my implementation of a perceptron algorithm, and then  investigate the algorithm's effectiveness on three types of datasets: linearly separable, not linearly separable, and multi-dimensional. As expected, the algorithm is able to find the decision boundary between linearly separable classes, yet fails with not linearly separable classes. Finally, the five dimensional data I generate does not appear to be  separable via a perceptron algorithm.\n\n### Perceptron Implementation\n\nYou can find my implementation of the perceptron algorithm at [Perceptron.py](https://github.com/leftmire/lizrightmireio.github.io/blob/main/posts/Perceptron/perceptron.py). \n\nPerceptron.grad first computes the score for each data point in the feature matrix. The score is the feature matrix X matrix multiplied by the weight vector.\n\nIf the score is a different sign from the actual class label, the update is computed. Else, zeros are returned -- this entry in the weight vector doesn't need to be updated.\n\n### Experiment 1: Linearly Separable\n\nWhen given linearly separable data, the perceptron algorithm should converge to a weight vector that describes a line separating the two classes of data. Let's check that this is true for our implementation of the perceptron algorithm.\n\nFirst, generate linearly separable data\n\ncode modified from [Prof Phil's Notes](https://www.philchodrow.prof/ml-notes/chapters/20-perceptron.html#fig-demonstration)\n\nIt is easy to imagine a decision boundary that would divide these two classes.\n\nNow, let's run the perceptron algorithm. After just 3 iterations, the loss is 0 and a dividing line is discovered.\n\nHow did this happen? Using a simple loop, the answer to this question can be visualized on side-by-side plots. The circled point is the one randomly chosen for the perceptron update, and the line updates from the dotted line to the solid line.\n\nvisualization code modified from [Prof Phil's Notes](https://www.philchodrow.prof/ml-notes/chapters/20-perceptron.html#fig-demonstration)\n\n### Experiment 2: Not Linearly Separable\n\nLet's generate some data that isn't linearly separable by increasing the noise of our original data\n\nNow, let's run the perceptron algorithm\n\nEven with 1,000 perceptron updates, the algorithm does not terminate because the loss will never reach 0!\n\nWhat dividing line does the perceptron algorithm produce, 1,000 updates later?\n\nHmm, this isn't even a very good separator. Obviously, the perceptron algorithm performs poorly on data that isn't perfect linearly separable.\n\n### Experiment 3: Multi-Dimensional Data\n\nThe perceptron algorithm should work on data that is more than 2-dimensional.\n\nFirst, geneate some five-dimensional data to experiment with.\n\nNow, run the perceptron algorithm.\n\nEven after 3,000 iterations, the loss stays fairly high -- around 0.3. The loss does not appear to be consistently decreasing with each update. This leads me to believe that the data I created isn't linearly separable.\n\n### Minibatch Perceptron Experiments\n\nI edited my perceptron.py file to be compatable with the mini-batch perceptron algorithm, which computes an update using k points at once rathar than a single point.\n\nNow, I will conduct experiments to explore how minibatch perceptron performs in different scenarios. First, set k = 1.\n\nWith k = 1, minibatch performs similarly to the regular perceptron. The loss tends to decrease until reaching 0.\n\nLet's try k = 10.\n\nWhen k = 10, minibatch perceptron can still find a separating line in 2d.\n\nNow, let's experiment with k = n -- the batch size is the entire data set\n\nWhen setting k = n with data that is not linearly separable, the minibatch perceptron algorithm converges with a loss close to 0.\n\n### Conclusion\n\nThe update in each iteration of the perceptron algorithm is computed as X \\* w, the product between a single row of matrix X and the weight vector w. Each row in X has p entries for the p features, so this process is O(p). \n\nThe mini-batch perceptron algorithm takes the dot product between each data point in the batch, so the runtime is O(kp) for each iteration.\n\nI was able to prove that the perceptron algorithm, though simple, is both effective and efficient at finding a linear classifier for data of any dimension. I wrote my first implementation of an algorithm and gained a deeper understanding of its inner workings. I then took my understanding further and implemented the more efficient mini-batch perceptron with improved convergence.\n\nThis knowledge will translate well when I study more complicated machine learning algorithms. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","title-block-banner":"../../img/landscape.png","title-block-banner-color":"white","theme":"cosmo","title":"Implementing the Perceptron Algorithm","author":"Liz Rightmire","date":"2024-03-27","image":"image.jpg","description":"Implementing and Testing the Perceptron Algorithm"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}